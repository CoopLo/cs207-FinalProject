{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9gbEZNpruFF"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) is a method for numerically finding the derivative of a function at a given point. It can be used to find derivatives of complex functions where computing the symbolic derivative can be impossible or computationally costly.\n",
    "\n",
    "Automatic differentiation is more accurate than other numerical differentiation methods such as the [finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method). The finite difference method attempts to find the derivative of a function at a given point by adding a small perturbation ($\\epsilon$):\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} \\approx \\frac{f(x+\\epsilon) - f(x)}{\\epsilon} $$\n",
    "\n",
    "When the perturbation is too large, the estimate for the derivative is not accurate. When the perturbation is too small, it starts to amplify floating point errors. \n",
    "\n",
    "Automatic differentiation achieves high accuracy while avoiding amplified floating point errors by (1) breaking down the function into a sequence of elementary functions (e.g., sin, cos, log, and exp), (2) calculating the exact derivation of these elementary functions, (3) and finally combining them using the [chain rule](https://en.wikipedia.org/wiki/Chain_rule), the [product rule](https://en.wikipedia.org/wiki/Product_rule), and simple mathematical operations (such as addition and multiplication). \n",
    "\n",
    "Given its speed and precision, automatic differentiation is popular within the field of computational science where it has numerous applications. This software package as an implementation of automatic differentiation using Python.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCO1uO16B3K2"
   },
   "source": [
    "# Background\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Automatic differentiation allows us to compute the true analytic derivative of a function to machine precision. At a high level, this is done by breaking down complex functions into their elementary components and propogating their derivative via the chain rule. Other methods of finding derivatives include symbolic differentiation and finite fifference. Symbolic differentiation is also accurate to machine precision but is more computationally costly and its implementation is more complex. Finite difference is easier and less costly to implement but can quickly lead to floating point errors. As such, automatic differentiation is a more precise and lightweight methodology to use.\n",
    "\n",
    "## Some Calculus\n",
    "\n",
    "The [product rule](https://en.wikipedia.org/wiki/Product_rule) is used to find the derivative of the product of two or more functions. In its simplest form, if $f$ and $g$ are functions, the derivative of their product is given by the following equation: \n",
    "\n",
    "$$ [f(x)g(x)]' = f'(x)g(x)+f(x)g'(x) $$\n",
    "\n",
    "The chain rule is used for computing the derivative of the composition of two or more functions. In its simplest form, if $f$ and $g$ are functions, the derivative of their composition is given by the following equation:\n",
    "\n",
    "$$ [f(g(x))]' = f'(g(x))*g'(x) $$\n",
    "\n",
    "## Modes of Automatic Differentiation\n",
    "\n",
    "The automatic differentiation method can be implemented in two ways depending on how the chain rule is utilized. Consider the formulation:\n",
    "\n",
    "$$ f(x) = g_3(g_2(g_1(x))) $$\n",
    "\n",
    "Using the chain rule, this function's derivative at point $x = a$ is computed as:\n",
    "\n",
    "$$ f'(a) = g_3'(.)*g_2'(.)*g_1'(a) $$\n",
    "\n",
    "The forward mode of automatic differentiation recursively propagates the calculated derivative from the right: first calculates $g_1'(a)$, then $g_2'(.)$, then $g_3'(.)$, and so on. \n",
    "\n",
    "The reverse mode of automatic differentiation recursively propagates the calculated derivative from the left: first calculates $g_3'(.)$, then $g_2'(.)$, then $g_1'(a)$, and so on.\n",
    "\n",
    "In our implementation, we will focus on the forward mode. \n",
    "\n",
    "## Forward Mode\n",
    "\n",
    "A useful tool associated with the forward mode is the computational trace. Using the computational trace, we can list the steps required to go from input values (the point at which the derivative is evaluated) to the input function.  \n",
    "\n",
    "Consider the following function:\n",
    "\n",
    "$$ f(x) = sin(e^{2x}) $$\n",
    "\n",
    "Say we want to evaluate the derivative of this function at $x = 5$. The steps for calculating the derivative using forward mode are given in the following table:\n",
    "\n",
    "| Trace | Elementary Function | Function Value | Derivative | Derivative Value |\n",
    "| :------: | :----------: | :-------: | :---------: | :--------: |\n",
    "| $x_{1}$ | $x$ | 5 | $\\dot{x}_{1}$ | $1$ |\n",
    "| $x_{2}$ | $2x_{1}$ | $10$ | $2\\dot{x}_{1}$ | $2$ |\n",
    "| $x_{3}$ | $e^{x_{2}}$ | $e^{10}$ | $e^{x_{2}}\\dot{x}_{2}$ | $2e^{10}$ |\n",
    "| $x_{4}$ | $sin(x_{3})$ | $sin(e^{10})$ | $cos(x_{3})\\dot{x}_{3}$ | $2e^{10}cos(e^{10})$ |\n",
    "\n",
    "We get the required derivative value $2e^{10}cos(e^{10})$.\n",
    "\n",
    "## Dual Numbers\n",
    "\n",
    "We utilize [dual numbers](https://en.wikipedia.org/wiki/Dual_number) in our implementation of the forward mode of automatic differentiation. Dual numbers are an extension to real numbers (similar to complex numbers). Dual numbers introduce a new element (typically represented by $\\epsilon$) with the useful property $\\epsilon^2 = 0$.  Using dual numbers and [Taylor series](https://en.wikipedia.org/wiki/Taylor_series), we can find the derivative of a function quickly. \n",
    "\n",
    "Say we have a function $f$ and we want to find its derivative at point $x = a$. We will set $x = a + b\\epsilon$ and find the Taylor expansion:\n",
    "\n",
    "$$ f(a+b\\epsilon) = \\sum_{n=0}^{\\infty} \\frac{(b\\epsilon)^nf^{(n)}(a)}{n!} = f(a) + b\\epsilon f'(a) $$\n",
    "\n",
    "All higher-order terms are equal to $0$ because of the dual number property $\\epsilon^2 = 0$. For function $f$, we get its derivative at point $x = a$ directly from the second term in the Taylor expansion.\n",
    "\n",
    "## Root Finding\n",
    "When the root of a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ is sought, we generally must use a root finding algorithm.\n",
    "Many functions have roots that can not be found analytically, or are simply too complicated to be able to find the root by hand. \n",
    "We therefore must resort to solving them computationally.\n",
    "One of the most popular methods for this is Newton's method.\n",
    "Newton's method for root finding incorporates gradient information so it is a perfect candidate to be implemented using automatic differentiation.\n",
    "The algorithm is as follows for a function $\\textbf{f}$, :\n",
    "\n",
    "> initialize $\\textbf{x}$ \\\\\n",
    "> for $k = 0, 1, 2, \\cdots $\n",
    ">> $\\textbf{x}_{k+1} = \\textbf{x}_k - \\textbf{J}_{f}^{-1}\\textbf{f}(\\textbf{x}_k)$\n",
    "\n",
    "This relatively simple algorithm is quite powerful, provided that the initial guess is close enough to the root.\n",
    "Newton's method fails when the derivative is 0, or when the initial guess is bad enough such that the current guess is farther from the root than the previous step.\n",
    "\n",
    "## Optimization\n",
    "Optimization is of great interest in many fields such as machine learning, physics, and engineering.\n",
    "There are many optimization algorithms out there, so we will only touch on a few.\n",
    "The algorithms we have implemented are all for functions $f: \\mathbb{R}^n \\to \\mathbb{R}$.\n",
    "We have chosen algorithms that require gradients and first derivative knowledge only.\n",
    "The first algorithm used is the classic gradient descent, seen below:\n",
    "> initialize $\\textbf{x}_0$ \\\\\n",
    "> for $k = 0, 1, 2, \\cdots$ \\\\\n",
    ">> $\\textbf{x}_{k+1} = \\textbf{x} - \\alpha_k \\nabla f(\\textbf{x}_k)$ \\\\\n",
    "\n",
    "where $\\alpha_k$ is the step size parameter.\n",
    "Gradient descent is quite popular due to its simplicity, however its convergence tends to be quite slow.\n",
    "\n",
    "The next algorithm implemented is the BFGS algorithm.\n",
    "This algorithm approximates the Hessian with gradient informatation, and is similar to Newton's method in the use of Hessian. It is as follows:\n",
    "\n",
    "> initalize $\\textbf{x}_0$ \\\\\n",
    "> initialize $\\textbf{B}_0$ hessian estimate \\\\\n",
    "> for $k = 0, 1, 2, \\cdots $ \\\\\n",
    ">> Solve $\\textbf{B}_k\\textbf{s}_k = -\\nabla f(\\textbf{x}_k)$ for $\\textbf{s}_k$ \\\\\n",
    ">> $\\textbf{x}_{k+1} = \\textbf{x}_k + \\textbf{s}_k$ \\\\\n",
    ">> $\\textbf{y}_k = \\nabla f(\\textbf{x}_{k+1}) - \\nabla f(\\textbf{x})$ \\\\\n",
    ">> $\\textbf{B}_{k+1} = \\textbf{B}_k + (\\textbf{y}_k\\textbf{y}_k^T)/(\\textbf{y}_k^T\\textbf{s}_k) - (\\textbf{B}_k\\textbf{s}_k\\textbf{s}_k^T\\textbf{B}_k)/(\\textbf{x}_k^T\\textbf{B}_k\\textbf{s}_k)$\n",
    "\n",
    "This method performs similarly to Newton's method for root finding, although is slower and less accurate due to Hessian approximation.\n",
    "\n",
    "The last method implemented is the conjugate gradient method.\n",
    "This method aims to approximate the Hessian without storing the approximation directly. It is as follows:\n",
    "> initialize $\\textbf{x}_0$ \\\\\n",
    "> $\\textbf{g}_0 = \\nabla f(\\textbf{x}_0)$ \\\\\n",
    "> $\\textbf{s}_0 = -\\textbf{g}_0$ \\\\\n",
    "> for $k = 0, 1, 2, \\cdots$ \\\\\n",
    ">> $\\textbf{x}_{k+1} = \\textbf{x}_k + \\alpha_k \\textbf{s}_k$ \\\\\n",
    ">> $\\textbf{g}_{k+1} = \\nabla f(\\textbf{x}_{j+1})$ \\\\\n",
    ">> $\\beta_{k+1} = (\\textbf{g}_{k+1}^T\\textbf{g}_{k+1})/(\\textbf{g}_k^T \\textbf{g}_k)$ \\\\\n",
    ">> $\\textbf{s}_{k+1} = -\\textbf{g}_{k+1} + \\beta_{k+1}\\textbf{s}_k$ \\\\\n",
    "\n",
    "\n",
    "This method performs similar to BFGS, although slightly worse.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQHuPuTFL8Z1"
   },
   "source": [
    "# How To Use\n",
    "\n",
    "## Installation via GitHub:\n",
    "\n",
    "Download package and install dependencies:\n",
    "\n",
    "*   `conda create -n test python=3.6`\n",
    "*   `conda activate test`\n",
    "*   `git clone https://github.com/make-AD-ifference/cs207-FinalProject.git`\n",
    "*   `cd cs207-FinalProject/`\n",
    "*   `pip install -r requirements.txt`\n",
    "*   `pytest`\n",
    "\n",
    "## Installation via PyPI:\n",
    "\n",
    "Download package (dependencies automatically installed):\n",
    "\n",
    "*   `conda create -n test python=3.6`\n",
    "*   `conda activate test`\n",
    "*   `pip install autodiff-kgl`\n",
    "\n",
    "## Importing the package in Python (an example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mUcO7eNdLWph"
   },
   "outputs": [],
   "source": [
    "from autodiff.autodiff import AutoDiff\n",
    "AD1 = AutoDiff(2,3)\n",
    "AD2 = AutoDiff.cos(AD1)\n",
    "AD3 = AutoDiff(5,[1,2])\n",
    "\n",
    "import autodiff.optimization\n",
    "import autodiff.root_finding\n",
    "import autodiff.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you get ModuleNotFoundError, please ensure you are using the same virtual environmnet where you ran the pip install command and using the Python interpreter from that virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmDjtKYeLXBQ"
   },
   "source": [
    "## Packaging Discussion\n",
    "In thinking through how to use our software package, we considered the following:\n",
    "\n",
    "#### Who are our users?\n",
    "We decided to target somewhat tech-savvy users who would be comfortable with a command line application as opposed to a more approachable and user-friendly GUI. Any single user should be able to install the package easily onto their machine without requiring broader deployment. \n",
    "\n",
    "#### Where should it run? \n",
    "We expect our package to be installed and run on any desktop devices through the command line. It should work regardless of the operating system as long as the user has Python on their machine (our assumption is that most users will have Python pre-installed on their Mac or Linux machines or will be able to easily download it otherwise).\n",
    "\n",
    "## Packaging Choice\n",
    "We plan to use pip as the package manager. We considered some other options such as conda install but ultimately chose pip because Python developers are already familiar with it and we did not want the installation process to be a barrier to using our program. We plan to register the name of our package on PyPI and upload source distributions via the setup.py file. To ensure that our package can be downloaded regardless of the user’s Python version, we plan to use pip instead of pip3. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTTAsbqkRJTF"
   },
   "source": [
    "# Software Organization\n",
    "\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "The directory structure looks like:\n",
    "\n",
    "`make-AD-ifference/`\n",
    "\n",
    ">`setup.py`\n",
    "\n",
    ">`.gitignore`\n",
    "\n",
    ">`.travis.yml`\n",
    "\n",
    ">`.requirements.txt`\n",
    "\n",
    ">`coverage.txt`\n",
    "\n",
    ">`README.md`\n",
    "\n",
    ">`LICENSE`\n",
    "\n",
    ">`autodiff/`\n",
    "\n",
    ">>`README.md`\n",
    "\n",
    ">>`__init__.py`\n",
    "\n",
    ">>`autodiff.py`\n",
    "\n",
    ">>`optimization.py`\n",
    "\n",
    ">>`root_finding.py`\n",
    "\n",
    ">>`plot.py`\n",
    "\n",
    ">`docs/`\n",
    "\n",
    ">>`documentation.ipynb`\n",
    "\n",
    ">>`milestone1.ipynb`\n",
    "\n",
    ">>`milestone2.ipynb`\n",
    "\n",
    ">`tests/`\n",
    "\n",
    ">>`README.md`\n",
    "\n",
    ">>`test_autodiff.py`\n",
    "\n",
    ">>`test_optimization.py`\n",
    "\n",
    ">>`test_root_finding.py`\n",
    "\n",
    ">>`test_plot.py`\n",
    "\n",
    ">`scratch/`\n",
    "\n",
    "\n",
    "**Note: Scratch folder contains scripts we wrote but did not end up using in our final implementation.**\n",
    "\n",
    "\n",
    "## Modules\n",
    "- `autodiff.py`: contains the autodifferentiation class `AutoDiff`, which implements Forward Mode AD. This module serves as our custom library that allows users to evaluate functions and their derivatives for each input value. The class also includes custom methods that our program supports. Each method returns the value and derivative of the specified function. See the implementation section for more details. \n",
    "\n",
    "- `optimization.py`: contains the algorithms for gradient descent method, BFGS method, and conjugate gradient method. Optimization only works for scalar to scalar and vector to scalar functions. \n",
    "\n",
    "- `root_finding.py`: contains newton's method for root finding. Root finding works for all vector to vector and scalar to scalar functions.  \n",
    "\n",
    "- `plot.py`: contains the plotting routine which uses the trace on top of a contour plot of the function. \n",
    "\n",
    "\n",
    "\n",
    "## Testing\n",
    "\n",
    "All testing is done using TravisCI, and evaluation of the tests is done using CodeCov. Our main test suite `test_autodiff.py` and extension test suites live in the `tests/` folder.\n",
    "\n",
    "- `test_autodiff.py`: contains our test suite for the `autodiff.py` module. It includes tests for scalar and vector functions.\n",
    "\n",
    "- `test_optimization.py`: contains our test suite for the `optimization.py` module.\n",
    "\n",
    "- `test_root_finding.py`: contains our test suite for the `root_finding.py` module.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q8-KmCY8xFyT"
   },
   "source": [
    "# Implementation\n",
    "\n",
    "\n",
    "## Class: AutoDiff\n",
    "\n",
    "We are using a simple one-class structure to implement forward mode autodifferentiation. The AutoDiff class keeps track of the trace value and the derivative of functions for each input variable. It has two attributes `val` and `der` which represent the value and the derivative respecitvely. We have a set of elementary functions that can easily be expanded. These elementary functions serve as building blocks for users to assemble the function they wish to evaluate.\n",
    "We provide users with an understandable interface that conveys which elementary functions our software supports. If a user enters a function that is a combination of any of these, our program is able to handle the input. Otherwise, we return a descriptive error and allow users to enter a new function.\n",
    "\n",
    "Elementary functions defined in the AutoDiff class:\n",
    "- Ln (natural log)\n",
    "- Log \n",
    "- Exp\n",
    "- Exp with any base (e.g. 2 ^ x)\n",
    "- Sin\n",
    "- Cos\n",
    "- Tan\n",
    "- arcsine\n",
    "- arccosine\n",
    "- arctangent\n",
    "- sinh\n",
    "- cosh\n",
    "- tanh\n",
    "- Sqrt\n",
    "- logistic\n",
    "\n",
    "Elementary mathematical operations supported (note: the cummutative properties of operations are preserved):\n",
    "- Addition\n",
    "- Subtraction\n",
    "- Multiplication\n",
    "- Division\n",
    "- Power\n",
    "\n",
    "Comparison operators:\n",
    "- equal\n",
    "- not equal\n",
    "- greater or equal\n",
    "- greater\n",
    "- less or equal\n",
    "- less\n",
    "\n",
    "Unary operator:\n",
    "- Negative\n",
    "\n",
    "\n",
    "\n",
    "## More Details about Attributes & Methods\n",
    "We have overloaded the methods in class `AutoDiff` to give the user flexibility in how functions are entered. Overloaded functions support elementary operations between two `AutoDiff` objects or 1 `AutoDiff` object and 1 scalar. This way, we also preserve the cummutative nature of functions where needed. As an example, the user may input f = 2x + 3 or f = 3 + 2x. Regardless of the order, the program returns the correct value and derivative. \n",
    "Each `AutoDiff` object has as attributes the value and the derivative, calculated using trace variables and elementary operations, for a given input value. \n",
    "\n",
    "More on the custom operations and functions supported in class `AutoDiff`:\n",
    "- `__init__`:  constructor of class `AutoDiff`. Initializes an `AutoDiff` object, setting `self.der` initially to 1. Takes in an input value `val` at which to evaluate the function value and derivative.\n",
    "- `__str__`: returns the string value of the function.\n",
    "- `__repr__`: returns the string value of the function.\n",
    "- `__eq__`: returns boolean, True if value of self is equal to other else False.\n",
    "- `__ne__`: returns boolean, True if value of self is not equal to other else False.\n",
    "- `__gt__`: returns boolean, True if value of self is greater than or equal to other else False.\n",
    "- `__ge__`: returns boolean, True if value of self is greater than other else False.\n",
    "- `__lt__`: returns boolean, True if value of self is less than or equal to other else False.\n",
    "- `__le__`: returns boolean, True if value of self is less than other else False.\n",
    "- `__add__`: overloaded addition function. Supports adding two `AutoDiff` objects or 1 `AutoDiff` object and 1 scalar.\n",
    "- `__radd__`: Supports addition of two `AutoDiff` objects or 1 `AutoDiff` object and 1 scalar regardless of input order. Ensures cummutative property of addition is preserved.\n",
    "- `__sub__`: overloaded subtraction function. Supports subtraction between two `AutoDiff` objects or 1 `AutoDiff` object and 1 scalar.\n",
    "- `__rsub__`: Supports subtraction of the form scalar - `AutoDiff` instead of `AutoDiff` - scalar.\n",
    "- `__mul__`: overloaded multiplication function. Supports multiplying two `AutoDiff` objects or 1 `AutoDiff` object and 1 scalar.\n",
    "- `__rmul__`: overloaded multiplication function. Supports multiplying two `AutoDiff` objects or 1 `AutoDiff` object and 1 scalar regardless of input order. Ensures cummutative property of multiplication is preserved.\n",
    "- `__truediv__`: overloaded division function. Supports dividing an `AutoDiff` object by another `AutoDiff` object or an `AutoDiff` object by a scalar.\n",
    "- `__rtruediv__`: Supports division of the form scalar / `AutoDiff` instead of `AutoDiff` / scalar.\n",
    "- `__pow__`: overloaded power function. Supports an `AutoDiff` object to the power of another `AutoDiff` object or an `AutoDiff` object to the power of a scalar.\n",
    "- `__rpow__`: overloaded power function. Supports an `AutoDiff` object to the power of another `AutoDiff` object or a scalar to the power of an `AutoDiff` object.\n",
    "- `__neg__`: returns negated `AutoDiff` object.\n",
    "- `sin`: returns sine of `AutoDiff` object.\n",
    "- `cos`: returns cosine of `AutoDiff` object.\n",
    "- `tan`: returns tangent of `AutoDiff` object.\n",
    "- `arcsine`: returns arcsine of `AutoDiff` object.\n",
    "- `arccosine`: returns arccosine of `AutoDiff` object.\n",
    "- `arctangent`: returns arctangent of `AutoDiff` object.\n",
    "- `sinh`: returns sinh of `AutoDiff` object.\n",
    "- `cosh`: returns cosh of `AutoDiff` object.\n",
    "- `tanh`: returns tanh of `AutoDiff` object.\n",
    "- `ln`: returns natural log of `AutoDiff` object.\n",
    "- `log`: returns log of `AutoDiff` object with base `base` as second input.\n",
    "- `exp`: returns exponential of `AutoDiff` object.\n",
    "- `expm`: returns exponential of `AutoDiff` object with base `base` as second input.\n",
    "- `logistic`: returns logistic of `AutoDiff` object.\n",
    "- `sqrt`: returns square root of `AutoDiff` object.\n",
    "\n",
    "## Using AutoDiff\n",
    "- Begin by initializing an AutoDiff object with a given value and derivative with respect to each variable: ( # variables = # columns in second argument)\n",
    "\n",
    "`x = AutoDiff(2, [3 , 1])`\n",
    "\n",
    "- Then, define the function in the following manner:\n",
    "\n",
    "`f = x ** 2`\n",
    "\n",
    "`f = AutoDiff.sin(x)`\n",
    "\n",
    "`f = AutoDiff.log(x, 2)`\n",
    "\n",
    "- The function's value can then be accessed as `f.val`\n",
    "- The function's derivative can be accessed as `f.der`\n",
    "\n",
    "## Using Root Finding\n",
    "\n",
    "The use of the Newton's method for root finding is quite simple. \n",
    "We simple declare the variables of interest, write a lambda function, and put those into the root finding routine.\n",
    "The function ouput must be a vector, even a single element vector, for the root finding to work.\n",
    "\n",
    "`x = AutoDiff(2, [1., 0.])`\n",
    "\n",
    "`y = AutoDiff(1, [1., 0.])`\n",
    "\n",
    "`func = lambda x: [x[0]**3 + x[1]**3]`\n",
    "\n",
    "`output = newton(func, [x, y])`\n",
    "\n",
    "## Using Optimization\n",
    "\n",
    "The optimization routines are similarly straightforward.\n",
    "The only significant change here, is that functions to be optimized return a scalar instead of a vector.\n",
    "\n",
    "`x = AutoDiff(2, [1., 0.])`\n",
    "\n",
    "`y = AutoDiff(1, [1., 0.])`\n",
    "\n",
    "`func = lambda x: 100*(x[1] - x[0]**2)**2 + x[0]**2`\n",
    "\n",
    "`output = BFGS(func, [x, y])`\n",
    "\n",
    "## Plotting\n",
    "\n",
    "The plotting script is designed to work directly with the output of the root finding or optimization routines.\n",
    "With the output of the optimization routine, we simply use the plotting function as\n",
    "`plot(func, output[3], output[1])`\n",
    "where `func` is the function we minimized.\n",
    "\n",
    "## External Dependencies\n",
    "- `Numpy`: In order to run our program, users have to import the numpy library as follows:\n",
    "`import numpy as np`. The Numpy library provides support for the elementary mathematical functions and operations handled by our program.\n",
    "\n",
    "- `matplotlib`: In order to use our plotting functionality, users have to import the numpy library as follows:\n",
    "`import matplotlib`. The matplotlib library provides support for plotting in Python.\n",
    "\n",
    "## Efficiency\n",
    "\n",
    "We have to consider things such as memory accesses, which can greatly speed up or slow down the code. We also have to consider numerical precision, although this shouldn’t be an issue with our hard-coding of functions and derivatives. Another possible consideration is memory overhead. Efficient storage of the functions and evaluations is crucial to making the code usable.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6E-SCpW7Hl0t"
   },
   "source": [
    "## Vector Valued Functions\n",
    "\n",
    "Our package now handles vector valued functions.\n",
    "Generally, we will be able to handle functions of the form\n",
    "$$ f: \\mathbb{R}^m \\to \\mathbb{R}^n $$\n",
    "for arbitrary $m$ and $n$.\n",
    "In order to accomplish this we will need the Jacobian matrix\n",
    "$$\\textbf{J} = \\left[\\begin{matrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots  & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}  \\end{matrix} \\right]$$\n",
    "\n",
    "\n",
    "## Our Extensions\n",
    "\n",
    "The extension is a root-finding and optimization suite, along with plotting functionality.\n",
    "For both procedures we implemented strictly gradient based methods.\n",
    "Methods such as Newton's method for optimization were excluded because they require second derivative information.\n",
    "Similarly, methods like the bisection method for root-finding were excluded because they required no gradient information.\n",
    "This left us with a fairly small subset of root-finding and optimization functions.\n",
    "An additional plotting tool is included to aid in visualization of the algorithms.\n",
    "\n",
    "\n",
    "For root-finding, we have Newton's method.\n",
    "For optimization, we have the gradient descent, BFGS, and conjugate gradient methods.\n",
    "In order to have standard Python usability, these methods are stored in\n",
    "`autodiff/root_finding.py` and `autodiff/optimization.py`.\n",
    "This structure allows us to call optimization methods as `autodiff.optimize.conjugat_gradient()`.\n",
    "\n",
    "The input syntax for each method is relatively straighforward, and based on the vector function implementation. In order to call an optimization routine, the user must simply define the variables of interest and function as such\n",
    "\n",
    "`x = ad(x, [1., 0.])`\n",
    "\n",
    "`y = ad(y, [0., 1.])`\n",
    "\n",
    "`fn = lambda x: x[0]**2 + y[0]**2`\n",
    "\n",
    "`output = autodiff.optimize.gradient_descent(fn, [x, y])`\n",
    "\n",
    "Paired with these, we have a plotting script.\n",
    "It takes in an optimization or root finding trace and plots that as a scatter plot over a filled contour plot of the function.\n",
    "If the routine ocnverged, a black star is plotted at the converged point. If not, no black star is plotted.\n",
    "An example of the conjugate gradient function plotted on top of the Rosenbrock function $f(x,y) = 100(y-x^2)^2 + (1-x)^2$ is seen below, both with and without convergence.\n",
    "\n",
    "Convergence\n",
    "\n",
    "![graph](https://drive.google.com/uc?id=1fWQyum_CiN8CJMIH1Iq6KKwi51wePg5v)\n",
    "\n",
    "\n",
    "\\\n",
    "\\\n",
    "No Convergence\n",
    "\n",
    "![graph](https://drive.google.com/uc?id=1oKQjvTnBgb64zNjqFiiISJNY0GzKcHVL)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "documentation.ipynb",
   "provenance": [
    {
     "file_id": "10lDqhhl1iYqBe15H7n-6F9bmyCxfbETM",
     "timestamp": 1574005170372
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
